{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n!pip install --no-deps cut_cross_entropy unsloth_zoo\n!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n!pip install --no-deps unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T02:03:10.911684Z","iopub.execute_input":"2025-03-10T02:03:10.911967Z","iopub.status.idle":"2025-03-10T02:03:30.740976Z","shell.execute_reply.started":"2025-03-10T02:03:10.911933Z","shell.execute_reply":"2025-03-10T02:03:30.739824Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Helpful functions used through the entire notebook\nimport torch\nimport torch.nn as nn\nfrom transformers import set_seed\nimport time\nimport inspect\nimport os\nmajor_version, minor_version = torch.cuda.get_device_capability()\nHAS_BFLOAT16 = (major_version >= 8)\nfrom inspect import currentframe as _C, getframeinfo\n_F = lambda c: getframeinfo(c).lineno # Gets line number\nWARN = lambda x: print(f\"\\033[31m{x}\\033[0m\") # Red colored warnings\n\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T02:03:30.742273Z","iopub.execute_input":"2025-03-10T02:03:30.742565Z","iopub.status.idle":"2025-03-10T02:03:34.923073Z","shell.execute_reply.started":"2025-03-10T02:03:30.742541Z","shell.execute_reply":"2025-03-10T02:03:34.922434Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import sys\n\ndef remove_patched_module(package_name):\n    modules_to_delete = [\n        name for name in sys.modules\n        if name == package_name or name.startswith(package_name + \".\")\n    ]\n    for name in modules_to_delete: del sys.modules[name]\n\nremove_patched_module(\"trl\")\nremove_patched_module(\"transformers\")\nremove_patched_module(\"peft\")\nremove_patched_module(\"bitsandbytes\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T02:03:34.925002Z","iopub.execute_input":"2025-03-10T02:03:34.925451Z","iopub.status.idle":"2025-03-10T02:03:34.931996Z","shell.execute_reply.started":"2025-03-10T02:03:34.925426Z","shell.execute_reply":"2025-03-10T02:03:34.931188Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from triton import jit\nimport triton\nimport triton.language as tl\n\n@triton.jit\ndef _your_dequantize_nf4_kernel(#pointers\n                                absmax2_ptr, out2_ptr, absmax1_ptr, out1_ptr, weight_ptr, code2_ptr, code1_ptr, offset1,\n                                #Dimensions\n                                blocksize1, blocksize2, num_elems_1, num_elems_2, shift1, shift2,\n                                #meta param\n                                BLOCK_SIZE: tl.constexpr, BLOCK_SIZE_2:tl.constexpr\n                                ):\n  \n    #init\n    pid = tl.program_id(axis=0)\n    block_start = pid * BLOCK_SIZE\n    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n    mask_for_abs2 = offsets < num_elems_2 \n    indexs = offsets >> shift2 \n    \n    #de_doublequant\n    absmax2_max = tl.load(absmax2_ptr + indexs,  mask = mask_for_abs2, other = 128)\n    absmax1_values = tl.load(absmax1_ptr + offsets, mask = mask_for_abs2, other = 128)\n    x = tl.load(code2_ptr + tl.cast(absmax1_values, tl.int32), mask = mask_for_abs2, other = 128)\n    val_offset = tl.fma(x, absmax2_max, tl.load(offset1))\n\n    tl.store(out2_ptr + offsets, val_offset, mask = mask_for_abs2, eviction_policy='evict_last')\n    tl.debug_barrier()\n     \n  \n    new_offsets = pid * BLOCK_SIZE_2 + tl.arange(0, BLOCK_SIZE_2)\n\n    #indexs = new_offsets >> shift1 \n    indexs = tl.inline_asm_elementwise(\n        asm =\"\"\"  \n        shr.b32 $0, $4, $8;\n        shr.b32 $1, $5, $9;\n        shr.b32 $2, $6, $10;\n        shr.b32 $3, $7, $11;\n        \"\"\",\n         constraints=(\n            \"=r,=r,=r,=r,\"\n            \"r,r,r,r,r,r,r,r\"),\n        args=[new_offsets, shift1],\n        dtype=(tl.int32),\n        is_pure=True,\n        pack=4,\n    )\n\n    mask_for_abs1 = new_offsets < num_elems_1\n    gathered_max = tl.load(out2_ptr + indexs, mask = mask_for_abs1, other = 128)\n    weights_values = tl.load(weight_ptr + new_offsets, mask = mask_for_abs1, other=128)\n    \n    #high_bits = weights_values >> 4\n    high_bits,  lowtem = tl.inline_asm_elementwise(\n        asm=\"\"\"\n        {\n          // --- Unpack the 32-bit input into 4 bytes ---\n          .reg .b8 a<4>;\n          // The input packed value is in $0.\n          mov.b32 {a0, a1, a2, a3}, $8;\n          \n          // --- Convert each 8-bit value to a 32-bit integer ---\n          cvt.u32.u8 $4, a0;\n          cvt.u32.u8 $5, a1;\n          cvt.u32.u8 $6, a2;\n          cvt.u32.u8 $7, a3;\n        }\n        // --- Compute high nibble for each (byte >> 4) ---\n        shr.b32 $0, $4, 4;\n        shr.b32 $1, $5, 4;\n        shr.b32 $2, $6, 4;\n        shr.b32 $3, $7, 4;\n        \n        // --- Compute low nibble for each (byte & 0x0F) ---\n        and.b32 $4, $4, 0x0F;\n        and.b32 $5, $5, 0x0F;\n        and.b32 $6, $6, 0x0F;\n        and.b32 $7, $7, 0x0F;\n        \n    \"\"\",\n         constraints=(\n            \"=r,=r,=r,=r,=r,=r,=r,=r,\"\n            \"r\"),\n        args=[weights_values],\n        dtype=(tl.int32, tl.int32),\n        is_pure=True,\n        pack=4,\n    )\n\n    x = tl.load(code1_ptr + high_bits, mask = mask_for_abs1, other=4)\n    hi_val = x * (gathered_max)\n\n    x = tl.load(code1_ptr + lowtem, mask = mask_for_abs1, other=4)\n    lo_val = x  * (gathered_max)\n\n    #storing\n    out_hi2 =  new_offsets  * 2\n    out_lo2 =  new_offsets  * 2 + 1\n    tl.store(out1_ptr + out_hi2, hi_val  , mask = mask_for_abs1 , eviction_policy='evict_last')\n    tl.store(out1_ptr + out_lo2, lo_val  , mask = mask_for_abs1 , eviction_policy='evict_last')\n  \n@torch.compile(fullgraph=True)\ndef _your_dequantize_nf4(weight, quant_state):\n    absmax2 = quant_state.state2.absmax \n    blocksize2 = quant_state.state2.blocksize\n    absmax1 = quant_state.absmax \n    out2 = torch.empty(absmax1.shape, dtype=quant_state.dtype, device=absmax1.device)\n    offset1 = quant_state.offset \n    out1 = torch.empty(quant_state.shape, dtype=quant_state.dtype, device=weight.device)\n    blocksize1 = quant_state.blocksize//2\n    num_elems_1 = out1.numel()\n    num_elems_2 = out2.numel()\n    code2 = quant_state.state2.code \n    code1 = quant_state.code\n    shift1 =  blocksize1.bit_length() - 1\n    shift2 = blocksize2.bit_length() - 1\n\n    grid = lambda meta: (triton.cdiv(num_elems_2, meta['BLOCK_SIZE']),)\n    \n    _your_dequantize_nf4_kernel[grid](\n        #wanna_be_pointers\n        absmax2 , out2 , absmax1 , out1 , weight,code2,code1, offset1,\n        #Dimensions\n        blocksize1,blocksize2,num_elems_1,num_elems_2,  shift1, shift2,\n        #meta_param\n        BLOCK_SIZE = 64, BLOCK_SIZE_2 = 64*32\n    )\n     \n    return out1#, kernal   \n\ndef your_dequantize_nf4(weight):\n    return _your_dequantize_nf4(weight.weight.data, weight.weight.quant_state)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T02:03:34.933390Z","iopub.execute_input":"2025-03-10T02:03:34.933588Z","iopub.status.idle":"2025-03-10T02:03:36.836267Z","shell.execute_reply.started":"2025-03-10T02:03:34.933570Z","shell.execute_reply":"2025-03-10T02:03:36.835628Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def dequantize_bnb_weight_temp_patched(weight: \"torch.nn.Parameter\", dtype: \"torch.dtype\", state=None):\n    \"\"\"\n    Helper function to dequantize 4bit or 8bit bnb weights.\n\n    If the weight is not a bnb quantized weight, it will be returned as is.\n    \"\"\"\n    if not isinstance(weight, torch.nn.Parameter):\n        raise TypeError(f\"Input weight should be of type nn.Parameter, got {type(weight)} instead\")\n\n    cls_name = weight.__class__.__name__\n    if cls_name not in (\"Params4bit\", \"Int8Params\"):\n        return weight\n\n    if cls_name == \"Params4bit\":\n        output_tensor = your_dequantize_nf4(weight.data, weight.quant_state)\n        logger.warning_once(\n            f\"The model is going to be dequantized in {output_tensor.dtype} - if you want to upcast it to another dtype, make sure to pass the desired dtype when quantizing the model through `bnb_4bit_quant_type` argument of `BitsAndBytesConfig`\"\n        )\n        return output_tensor.to(dtype)\n\n    if state.SCB is None:\n        state.SCB = weight.SCB\n\n    if hasattr(bnb.functional, \"int8_vectorwise_dequant\"):\n        # Use bitsandbytes API if available (requires v0.45.0+)\n        dequantized = bnb.functional.int8_vectorwise_dequant(weight.data, state.SCB)\n    else:\n        # Multiply by (scale/127) to dequantize.\n        dequantized = weight.data * state.SCB.view(-1, 1) * 7.874015718698502e-3\n\n    return dequantized.to(dtype)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T02:03:36.837112Z","iopub.execute_input":"2025-03-10T02:03:36.837429Z","iopub.status.idle":"2025-03-10T02:03:36.842591Z","shell.execute_reply.started":"2025-03-10T02:03:36.837408Z","shell.execute_reply":"2025-03-10T02:03:36.841881Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\nimport transformers.integrations.bitsandbytes\ntransformers.integrations.bitsandbytes.dequantize_bnb_weight = dequantize_bnb_weight_temp_patched\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T02:03:36.843469Z","iopub.execute_input":"2025-03-10T02:03:36.843689Z","iopub.status.idle":"2025-03-10T02:03:39.806473Z","shell.execute_reply.started":"2025-03-10T02:03:36.843667Z","shell.execute_reply":"2025-03-10T02:03:39.805701Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import os\nimport torch\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,\"\\\n    \"roundup_power2_divisions:[32:256,64:128,256:64,>:32]\"\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom trl import SFTTrainer, SFTConfig\nfrom datasets import load_dataset\nimport os\nmax_seq_length = 2048\ntorch.set_default_dtype(torch.float16)\nmodel_name = \"unsloth/meta-Llama-3.1-8B-Instruct-bnb-4bit\"\ndtype = torch.float16\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit              = True,\n    bnb_4bit_use_double_quant = True,\n    bnb_4bit_quant_type       = \"nf4\",\n    bnb_4bit_compute_dtype    = dtype,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map = \"auto\",\n    attn_implementation = \"sdpa\",\n    quantization_config = bnb_config,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.padding_side = \"right\"\n\nlora_config = LoraConfig(\n    r = 64,\n    lora_alpha = 128,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout = 0,\n    bias = \"none\",\n    task_type = TaskType.CAUSAL_LM,\n)\n\n# Get LoRA and setup model\nmodel = get_peft_model(model, lora_config)\nwith torch.no_grad():\n    for name, param in model.named_parameters():\n        if \".lora_A.\" in name or \".lora_B.\" in name: param.requires_grad_(True)\n        else: param.requires_grad_(False)\nmodel.gradient_checkpointing_enable()\nmodel.enable_input_require_grads()\n\n# Get dataset\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, SFTConfig\nurl = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\ndataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train[:10%]\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T02:03:39.807431Z","iopub.execute_input":"2025-03-10T02:03:39.807836Z","iopub.status.idle":"2025-03-10T02:04:36.389033Z","shell.execute_reply.started":"2025-03-10T02:03:39.807810Z","shell.execute_reply":"2025-03-10T02:04:36.388384Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c0f3da4b2544765bf587afa404cd5f0"}},"metadata":{}},{"name":"stderr","text":"Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"651714de22e94ea79e4f7a0c2faa9b18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44a295ef39ae4939b44ec6111c753de6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cbca8f1b6d14133a283ccccae915002"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a2331bf45974e799c29dac467c1c138"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb0f687cd85b4a849ec6a0ecd496b981"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unified_chip2.jsonl:   0%|          | 0.00/95.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ea1bf8c71a7489f834ffa4a13864c07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba57afbe46314fe784a536dc06c81be2"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"fsdp_config = {\n  \"compute_environment\": \"LOCAL_MACHINE\",\n  \"debug\": False,\n  \"distributed_type\": \"FSDP\",\n  \"downcast_bf16\": \"no\",\n  \"enable_cpu_affinity\": False,\n  \"fsdp_config\": {\n    \"fsdp_activation_checkpointing\": True,\n    \"fsdp_auto_wrap_policy\": \"TRANSFORMER_BASED_WRAP\",\n    \"fsdp_backward_prefetch\": \"BACKWARD_PRE\",\n    \"fsdp_cpu_ram_efficient_loading\": True,\n    \"fsdp_forward_prefetch\": True,\n    \"fsdp_offload_params\": True,\n    \"fsdp_sharding_strategy\": \"FULL_SHARD\",\n    \"fsdp_state_dict_type\": \"SHARDED_STATE_DICT\",\n    \"fsdp_sync_module_states\": True,\n    \"fsdp_use_orig_params\": True\n  },\n  \"machine_rank\": 0,\n  \"main_process_ip\": \"\",\n  \"main_process_port\": 8000,\n  \"main_training_function\": \"main\",\n  \"mixed_precision\": \"fp16\",\n  \"num_machines\": 2,\n  \"num_processes\": 2,\n  \"rdzv_backend\": \"static\",\n  \"same_network\": True,\n  \"tpu_env\": [],\n  \"tpu_use_cluster\": False,\n  \"tpu_use_sudo\": False,\n  \"use_cpu\": False\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T02:04:36.389768Z","iopub.execute_input":"2025-03-10T02:04:36.390006Z","iopub.status.idle":"2025-03-10T02:04:36.394755Z","shell.execute_reply.started":"2025-03-10T02:04:36.389985Z","shell.execute_reply":"2025-03-10T02:04:36.393924Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"training_args = SFTConfig(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 1,\n        max_steps = 10,\n        logging_steps = 1,\n        output_dir = \"outputs\",\n        seed = 3407,\n        max_seq_length = max_seq_length,\n        fp16 = model.get_input_embeddings().weight.dtype == torch.float16,\n        bf16 = model.get_input_embeddings().weight.dtype == torch.bfloat16,\n        report_to = \"none\", # For W&B\n        dataset_num_proc = 4,\n       \n        fsdp_config=fsdp_config\n    )\n\ntrainer = SFTTrainer(\n    model = model,\n    train_dataset = dataset,\n    processing_class = tokenizer,\n    args =training_args,\n)\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T02:04:36.396760Z","iopub.execute_input":"2025-03-10T02:04:36.396986Z","iopub.status.idle":"2025-03-10T02:06:02.911958Z","shell.execute_reply.started":"2025-03-10T02:04:36.396965Z","shell.execute_reply":"2025-03-10T02:06:02.911039Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Converting train dataset to ChatML (num_proc=4):   0%|          | 0/21029 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fc3ba347fe2426394acb68bee7fdaf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset (num_proc=4):   0%|          | 0/21029 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"658f678fee1c4520bccb7abf087e1f29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset (num_proc=4):   0%|          | 0/21029 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7243c3b46fb14862b6cbbfe4a3394b15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset (num_proc=4):   0%|          | 0/21029 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"335ba70ddf7849888c06e58b8cbae914"}},"metadata":{}},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 01:06, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>9.084200</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>9.077300</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>11.471200</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>8.708600</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>7.755100</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>7.194500</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>6.528000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>7.011900</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>6.294900</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>7.448900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=10, training_loss=8.057448768615723, metrics={'train_runtime': 74.4421, 'train_samples_per_second': 1.075, 'train_steps_per_second': 0.134, 'total_flos': 461650822987776.0, 'train_loss': 8.057448768615723})"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}